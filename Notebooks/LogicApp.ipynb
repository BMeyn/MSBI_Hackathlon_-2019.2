{
  "cells": [
    {
      "metadata": {
        "collapsed": true
      },
      "cell_type": "markdown",
      "source": "![title](https://miro.medium.com/max/400/1*2kYdgcRC9yn0Arr5vjAWeg.png)"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "***"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Azure Logic Apps"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Mit den Azure logic Apps können wir verschiedene Services oder Programmabläufe in einem Workflow anordnen und ausführen lassen. Die Ausführung kann dabei durch verschiedene Trigger ausgelöst werden.\n\n- Http Trigger\n- Blob Storage Trigger\n- TimerTrigger \n- CosmosDBTrigger\n- QueueTrigger \n- EventHubTrigger \n- ServiceBusQueueTrigger\n- ServiceBusTopicTrigger\n\nWir werden den BlobStorage Trigger verwenden, um die Logic App auszulösen wenn ein Bild in unseren Blob Storage geschrieben wird. Anschließend wollen wir eine Vorhersage durch unser erstelltes Custom Vision Modell ausführen. Da wir prozesse auch parallelisieren können wir gleichzeitig eine Vorhersage des Computer Vision Services auf das hochgeladene Bild anwenden. Anschließend müssen wir noch die JSON Rückgaben bereinigen und die Ergebnisse der beiden Modelle zusammenführen. \n\nDas schöne ist, dass wir die Logik App und alle Verbindungen über das Azure Portal konfigurieren können.\n\nMit den folgenden Schritten können wir die Logik App erstellen.\n\n1. Über das Azure Portal auf die Logik App zugreifen\n2. Erstellen eine Bolb Storage Triggers\n3. Implementieren einer Vorhersage des Custom Vison Models\n4. Testen der Logik App\n5. Implementieren des Computer Vision OCR Services\n6. Prasen der Json ausgaben\n7. Zusammen führen der Json ausgaben und Speichern der Ergebnisse\n\nHier fndest du noch weiter Informationen zu den Azure Logik Apps\n\n- [Übersicht: Was ist Azure Logic Apps?](https://docs.microsoft.com/de-de/azure/logic-apps/logic-apps-overview)\n- [Schnellstart: Erstellen Ihres ersten automatisierten Workflows mit Azure Logic Apps – Azure-Portal](https://docs.microsoft.com/de-de/azure/logic-apps/quickstart-create-first-logic-app-workflow)\n- [Erstellen Ihrer ersten Funktion mit Visual Studio](https://docs.microsoft.com/de-de/azure/azure-functions/functions-create-your-first-function-visual-studio)\n- [Erstellen Ihrer ersten Funktion mit Visual Studio Code](https://docs.microsoft.com/de-de/azure/azure-functions/functions-create-first-function-vs-code)\n- [Schnellstart: Erstellen und Veröffentlichen einer Funktion in Azure Functions mithilfe von Java](https://docs.microsoft.com/de-de/azure/azure-functions/functions-create-first-java-maven)\n- [Erstellen Ihrer ersten PowerShell-Funktion in Azure (Vorschauversion)](https://docs.microsoft.com/de-de/azure/azure-functions/functions-create-first-function-powershell)\n\n- [Erstellen einer durch HTTP ausgelösten Funktion in Azure](https://docs.microsoft.com/de-de/azure/azure-functions/functions-create-first-function-python)\n"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "***"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "## <font color='#0066cc'></font>\n\n<img src=\"../media/CustomVisionImages/CustomVisionCreateProject.gif\" width=\"1000\">\n",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (<ipython-input-5-f04ccbf367cf>, line 3)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-5-f04ccbf367cf>\"\u001b[0;36m, line \u001b[0;32m3\u001b[0m\n\u001b[0;31m    <img src=\"../media/CustomVisionImages/CustomVisionCreateProject.gif\" width=\"1000\">\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## <font color='#0066cc'>Erstellen eine Bolb Storage Triggers</font>\n\n\nName = blostorage container!!!!"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Navigiere über das Azure Portal in deine in die Erstellte Resourcen gruppe und wähle die Azure Logic App. Auf der Übersicht Seite der Logic App wirst du ein Infromationsvideo zu dem Service sowie eine Auswahl von verschiedenen Triggern entdecken.\n\n\n- Wähle die Erstellund einer Leeren Logic App Vorlage\n- Gebe in des Suchfeld \"BlobStorage\" ein ein wähle einen Verbindungsnamen sowie einen BlobStorage mit dem du dich verbinden möchtest\n- Wähle die Zugriffsrate auf den BlobStorage (zum testen habe ich jeden Minute gewählt) und klicke auf \"Next Step\" um den Trigger zu erstellen\n- Speichere den Logic App Workflow bevor du zum nächsten SChritt weiter gehst"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "<img src=\"../media/LogicAppImages/BlobTrigger.gif\" width=\"1000\">"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "***"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## <font color='#0066cc'>Implementieren einer Vorhersage des Custom Vison Models</font>"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "markdown",
      "source": "Um das Custom Vison Modell in unseres Workflow zu integrieren müssen wir zunächst einen Connector zu der Prediction API erstellen. Dies wir automatisch abgefrag wenn wir in unseren Workflow ein Custom Vision element einfügen. Anschließend müssen wir unser Projekt und das bereitgestellte Modell, sowie den Bild path definieren, um eine Vorhersage zu erhalten"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "markdown",
      "source": "<img src=\"../media/LogicAppImages/CustomVision.gif\" width=\"1000\">"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "***"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## <font color='#0066cc'>Testen der Logik App</font>"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "markdown",
      "source": "Wir können die Logik App auf zwei wege teste. Zunächst müssen wir die Logik App starten. Danach können wir entweder die bereits erstellte PowerApp nutzen um die Bilder Hochzuladen oder wir laden die Bilder direkt in den BlobStorage.\nDa ich es hier besser darstellen kann, zeige ich euch den Test mit dem direkten Upload in den BlobStorage. Ich würde an euerer Stelle aber beide Varianten testen, um sicher zu gehen das alle Schritte funktionieren.\n\nFalls ihr Probleme mit der Custm Vison Schnittstelle habt schaut einfach mal, ob die Zugriffskontrolle eures Bobstorages richtig konfiguriert ist.\n"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "markdown",
      "source": "<img src=\"../media/LogicAppImages/CustomVisionTest.gif\" width=\"1000\">"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "***"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## <font color='#0066cc'>Implementieren des Computer Vision OCR Services</font>"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Für die Erstellung und die Auswahl des richtigen Computer Vision Services ist ein wenig vorwissen notwending. \n\nMicrosoft bietet grundsätzlich 2 verschiedene OCR Modelle an. Das OCR Modell ist für die Erkennung von Computergeschrieben Texten und das recognizeText Text Modell kann für die Erkennung von Handgeschriebenen texten genutzt werden. Wir werden das letztere für unsere Vorhersagen nutzen, da das OCR Model nur sehr schlechte Ergebnisse liefert. Dies kann verschiedene Gründe haben\n\n- Wölbung auf der Drehscheide der Zahlen verzerrt die die Zahlen \n- Es die auf den zählern dargestellten Fonts weichen zu start von den Trainingsdaten ab\n- Schlecht Qualität der Bilder \n- Andere Font größe der dargestellten Bilder\n\nNach mehreren Test habe ich aber festgestellt das die recognizeText also die Handschrifterkennung besser zu unseren Daten passt und wir diese Nutzen können.\n\n\nDies Führt allerdings dazu das wir nicht die Vorgefertigten LogikApp Templates für Computer Vision nehmen könnne, da die recognizeText Funktion noch nicht integriert ist. Dies können wir aber einfach umgehen indem wir ein HTTP Template nutzen und den Service zunächst mt einem POST und dannach mit einem GET request ansprechen. Nachdem wir unsere Zugangsschlüssen zu dem Service gesendet haben, können wir mit dem GET request die Vohersage abfragen.\n\n"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### <font color='#0066cc'>POST request Computer Vision OCR Services</font>"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "\nDazu müssen wir unserem Endpoint und in dem Queries unserer Anfrage zusätzlich parameter mitgeben.\n\n\nWerte für die Definition der HTTP Anfrage:\n\nURI:      Comuter Vision ENDPOINT + /vision/v2.0/recognizeText                              \n\n|          |      key                       |  value                                          |\n|----------|--------------------------------|-------------------------------------------------|\n| Header:  |    OCP-Apim-Subscription-Key   |   your-Computer-Vision-Key                      |\n| Queries: |    mode                        |   Handwritten                                   |\n\n\nBody:   \n{                                                                                 \n            \"url\": \" your-storage-account-url @{triggerBody()?['Path']}\"}                            \n            } \n\nUm ein besseres Verständis zu den Computer Vision Services zu erhalten und die HTTP anfragen kennzulernen kannst du in dem folgenen Link einige [Cognetive Services Testen](https://azure.microsoft.com/de-de/services/cognitive-services/computer-vision/).\n\nDie HTTP Anfragen und deren Parameter kannst du dir [hier](https://westus.dev.cognitive.microsoft.com/docs/services/5adf991815e1060e6355ad44/operations/56f91f2e778daf14a499e1fa) anschauen."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "<img src=\"../media/LogicAppImages/ComputerVision.gif\" width=\"1000\">"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### <font color='#0066cc'>Einbauen eines Delays in die Logic APP</font>"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Da wir im nächsten Schritt auf den Header zugreifen müssen um den Operation-Location key von dem Service zu erhalten, müssen wir einen Delay einbauen, um auf eine Antwort von dem Server zu warten."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "<img src=\"../media/LogicAppImages/AddDelay.gif\" width=\"1000\">"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### <font color='#0066cc'>Einbauen einer Condition</font>"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Neben dem Deploy können wir auch noch eine Condition einbauen, um unsere Logic APP zu beenden und evlt. eine Email zu senden das der Server nicht mehr errichbar ist"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "<img src=\"../media/LogicAppImages/AddCondition.gif\" width=\"1000\">"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### <font color='#0066cc'>Parsen der Header aus der POST Request Rückgabe</font>"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Im nächsten schritt müssen wir nur noch den Header von der Rückgabe des POST Requests Parsen damit wir auf den Operation-Location key in der GET Abfrage zugreifen können. Ihr könnt die Logik APP einfach einmal laufen lassen und euch die Ausgabe aus dem POST request anschauen bzw. kopieren und müsst dann nur noch die Datentypen für das Shema definieren.\n\nOder du nimmst einfach das Shema das ich dir nach dem GIF bereitstelle. ;D"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "<img src=\"../media/LogicAppImages/AddParseJson.gif\" width=\"1000\">"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "{\n    \"Cache-Control\": {\n        \"type\": \"string\"\n    },\n    \"Content-Length\": {\n        \"type\": \"string\"\n    },\n    \"Date\": {\n        \"type\": \"string\"\n    },\n    \"Expires\": {\n        \"type\": \"string\"\n    },\n    \"Operation-Location\": {\n        \"type\": \"string\"\n    },\n    \"Pragma\": {\n        \"type\": \"string\"\n    },\n    \"Strict-Transport-Security\": {\n        \"type\": \"string\"\n    },\n    \"X-AspNet-Version\": {\n        \"type\": \"string\"\n    },\n    \"X-Powered-By\": {\n        \"type\": \"string\"\n    },\n    \"apim-request-id\": {\n        \"type\": \"string\"\n    },\n    \"x-content-type-options\": {\n        \"type\": \"string\"\n    }\n}"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### <font color='#0066cc'>Abfragen der Modellausgaben durch einen GET Request</font>"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Nun können wir die Rükgabe aus dem vorherigen POST Request nutzen und auf die Uniform Resource Identifier (URI) aus der Operation-Location zugreifen. Mit einem GET Abfragen werden wir nun auf diese Resource zugreifen und nach der Modellausgabe fragen.\n\nDa dies eine eher kreative Lösung ist (weil Microsoft die Handschrifterkennung für die Logik APps leider noch nicht implementiert hat) müssen wir den Zugriff auf die Operation-Location URI mit ein wenig Code definieren. Wir wollen also auf den Body der vorherigen Aktion (Parse_JSON) zugreifen und aus der JSON die varable Operation-Location auslesen. Wenn du den fogenden Code in die HTTP GET Abfrage einfügst, kannst du dennoch die Variable definieren."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "@{body('Parse_JSON')?['Operation-Location']}"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Ich hoffe du kannst immernoch folgen und falls nicht kann ich dir nur empfehlen die Logik App einmal auszuführen und dir die Ausgaben der einzelnen Arbeitsschritte einmal genauer anzusehen. Das wir dir bestimmt zu einem besseren Verständniss verhelfen."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "<img src=\"../media/LogicAppImages/HTTPGET.gif\" width=\"1000\">"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## <font color='#0066cc'>Test der Computer Vision Logik</font>\n\nWenn du nun die Logik App erneut testest, solltest du nach der HTTP GET Abfrage einen Output mit verschiedenen \"bounding-boxen\" und den dazugehörigen Texten erhalten.\n\nFalls nicht, helfe ich dir gerne!"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "***"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## <font color='#0066cc'>Erweitern der Logik für die Auswahl der besten Objekterkennungs Vorhersagen</font>"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Da wir nun auch eine Ausgabe für die Erkennung der Zahlen und Buchstaben erhalten, können wir damit beginnen die beiden Ergebnisse miteinander zu verbinden. Zu erst müssen wir für unsere Objekterkennung die Recheckbox mit der höchsten Vorhersagewahrscheinlichkeit herausfinden. Genau dieses Recheck sollten, wenn das Modell richtig liegt, unser Zahlenbereich sein.\n\nDazu lernen wir einige neue Element der Logic Apps kennen:\n\n- Variabeln: Damit können wir Ausgaben mit verschiedenen Datentypen speichern und wieder aufrufen\n- ForEach Schleifen: Mit Schleifen können wir durch alle Elemente eines Array iterieren und auf die Elemente Operationen durchführen\n- Append to Array Vaiable: Wir können an eine bestehende Vaiable einen neuen Wert anhängen.\n\nMit diesen Elementen könnn wir die beste Vorhersage aus unserem Objekterkennungsergebnisse herausfiltern.\n\n- Erstellen einer Array Variable in der alle Modell Vorhersagen über 0.8 gespeichert werden\n- Erstellen eine Foreach loops, um durch alle Vorhersagen des Objekterkennung Modells zu iterieren\n- Parse JSON für jedes ForEachLoop item, damit wir auf die Genauigkeiten zugreifen können\n- Einfügen einer Condition bei der wir alle Genauigkeiten mit einem Wert von größer als 0.8 nach true sortieren\n"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "markdown",
      "source": "##### <font color='#0066cc'>Erstellen einer Array Variable</font>"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "<img src=\"../media/LogicAppImages/init_var.gif\" width=\"1000\">"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "markdown",
      "source": "##### <font color='#0066cc'>Erweitern der Logik für die Auswahl der besten Objekterkennungs Vorhersagen</font>\n\nHier ist das benötigte Json Shema. Wie immer könnt ihr euch es auch selbst erstellen lassen wenn ihr den Output von Custom Vison anschaut bzw. die Option in dem Parser verwendet. "
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "{\n    \"properties\": {\n        \"boundingBox\": {\n            \"properties\": {\n                \"height\": {\n                    \"type\": \"number\"\n                },\n                \"left\": {\n                    \"type\": \"number\"\n                },\n                \"top\": {\n                    \"type\": \"number\"\n                },\n                \"width\": {\n                    \"type\": \"number\"\n                }\n            },\n            \"type\": \"object\"\n        },\n        \"probability\": {\n            \"type\": \"number\"\n        },\n        \"tagId\": {\n            \"type\": \"string\"\n        },\n        \"tagName\": {\n            \"type\": \"string\"\n        }\n    },\n    \"type\": \"object\"\n}",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 3,
          "data": {
            "text/plain": "{'properties': {'boundingBox': {'properties': {'height': {'type': 'number'},\n    'left': {'type': 'number'},\n    'top': {'type': 'number'},\n    'width': {'type': 'number'}},\n   'type': 'object'},\n  'probability': {'type': 'number'},\n  'tagId': {'type': 'string'},\n  'tagName': {'type': 'string'}},\n 'type': 'object'}"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "<img src=\"../media/LogicAppImages/for_each_parse.gif\" width=\"1000\">"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "markdown",
      "source": "##### <font color='#0066cc'>Erweitern der Logik für die Auswahl der besten Objekterkennungs Vorhersagen</font>"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "<img src=\"../media/LogicAppImages/condition.gif\" width=\"1000\">"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "##### <font color='#0066cc'>Testen der eingefügten Logik</font>"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Achtet beim testen der Logik darauf, dass ihr die einzelnen Elemente der ForEachScheife durchgehen könnt. Wundert euch also nicht, wenn in der variable zunächst kein Wert erscheint."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## <font color='#0066cc'>Parsen der Computer Vision Json ausgaben</font>"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "markdown",
      "source": "Im nächsten Schritt müssen wir die Json Ausgaben Parsen damit wir diese weiter verarbeiten können. Dazu müsse wir das Shema der Computer Vision Ausgaben definieren. Es Spart ein wenig Zeit, wenn man die Logik App einmal laufen lässt und dann die Ausgabe von dem HTTP Trigger anzeigen lässt. Du musst dann nurnoch den output kopieren und die richtigen Daten Typen definieren.\n"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "markdown",
      "source": "<img src=\"../media/LogicAppImages/jsonparse.gif\" width=\"1000\">"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "***"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## <font color='#0066cc'>Zusammen führen der Json ausgaben und Speichern der Ergebnisse</font>"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "***"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## <font color='#0066cc'>Nächsten Schritte</font>"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python36",
      "display_name": "Python 3.6",
      "language": "python"
    },
    "varInspector": {
      "window_display": false,
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "library": "var_list.py",
          "delete_cmd_prefix": "del ",
          "delete_cmd_postfix": "",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "library": "var_list.r",
          "delete_cmd_prefix": "rm(",
          "delete_cmd_postfix": ") ",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ]
    },
    "nbTranslate": {
      "hotkey": "alt-t",
      "sourceLang": "en",
      "targetLang": "fr",
      "displayLangs": [
        "*"
      ],
      "langInMainMenu": true,
      "useGoogleTranslate": true
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}